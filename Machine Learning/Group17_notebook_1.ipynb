{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b9ed09-fbf3-458a-ab98-66167ea3fedb",
   "metadata": {},
   "source": [
    "# <center>Project of Machine Learning</center>\n",
    "\n",
    "<center>\n",
    "Master in Data Science and Advanced Analytics <br>\n",
    "NOVA Information Management School\n",
    "</center>\n",
    "\n",
    "** **\n",
    "## <center>*TO GRANT OR NOT TO GRANT: DECIDING ON COMPENSATION BENEFITS*</center>\n",
    "\n",
    "<center>\n",
    "Group 17 <br>\n",
    "Diogo Ruivo, 20240584  <br>\n",
    "José Tiago, 20240582  <br>\n",
    "Matilde Miguel, 20240549  <br>\n",
    "Nuno Sousa, 20222125  <br>\n",
    "Rafael Lopes, 20240588  <br>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "</center>\n",
    "\n",
    "\n",
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f63c20-f8b6-4740-a705-781818ca94c5",
   "metadata": {},
   "source": [
    "# Agreement Reached\n",
    "In this notebook we create the model for the 'Agreement Reached' feature so it can be incorporated into the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12a8e03-e23a-4622-9772-6ccffc366f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statistics\n",
    "\n",
    "#data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#empty values\n",
    "import missingno as msno\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f10cad-56ad-43c6-bd06-9451b07b2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6658a-b833-49f8-823d-c27e1b264201",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782f6d91-6fec-4887-a643-7c90765d13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fdcea-14e6-44dd-8cbe-06a597c50c93",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bbe1b-c610-45e3-88b1-64b46be3e8e3",
   "metadata": {},
   "source": [
    "set Claim Identifier as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2375a3-5e78-444c-8b0e-b298146d5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Claim Identifier', inplace=True)\n",
    "test.set_index('Claim Identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd10e5-a067-4609-85c0-0bd5dbe18da8",
   "metadata": {},
   "source": [
    "#### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c78386-4b75-4816-afe8-625c58042dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8c5ca-e956-49f7-a629-2319ec9a143b",
   "metadata": {},
   "source": [
    "#### Drop Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93842c13-818c-4654-8da4-1548e4a21e97",
   "metadata": {},
   "source": [
    "WCB Decision, Claim Injury Type are not in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cbc587-0038-450a-9a25-49002a681617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['WCB Decision'], inplace = True, axis = 1)\n",
    "train.drop(['Claim Injury Type'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30c708-3953-4eab-9514-8d785ca6ca2f",
   "metadata": {},
   "source": [
    "the column don't have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4837a61b-d7d3-4668-a704-efeaa1b347a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['OIICS Nature of Injury Description'], inplace=True, axis=1)\n",
    "test.drop(['OIICS Nature of Injury Description'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b7b40-0d4e-4439-9ebe-02886227e99d",
   "metadata": {},
   "source": [
    "#### Drop Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff593aa-1908-434f-9d30-346ee295ef61",
   "metadata": {},
   "source": [
    "we eliminate the lines that do not have Agreement Reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5267a5-0e5f-40fa-a3c6-ccdc5b44ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=['Agreement Reached'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be02ec-8f83-46b8-a8c8-fc8a27b0c980",
   "metadata": {},
   "source": [
    "we eliminate rows with only 1, 2 or 3 NaN values, as we see that 'C-3 Date', 'First Hearing Date' and 'IME-4 Count' columns have +- 70% of the values ​​missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167d7031-3274-41b6-8e32-2d4cd4111ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(thresh=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58805e59-9f6d-4f96-bced-a923a56a068d",
   "metadata": {},
   "source": [
    "#### Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dfe835-81f0-4db1-b105-685c605749dc",
   "metadata": {},
   "source": [
    "we had a code wrongly labeled, so we change it to have the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538853e4-d8de-4638-929a-c00c5e1bbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WCIO Part Of Body Code'] = train['WCIO Part Of Body Code'].apply(lambda x: 0 if x < 0 else x)\n",
    "test['WCIO Part Of Body Code'] = test['WCIO Part Of Body Code'].apply(lambda x: 0 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79863561-2390-413b-bff5-27e2de562285",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "int_cols = ['Age at Injury', 'Birth Year', 'IME-4 Count', 'Number of Dependents']\n",
    "float_to_object = ['Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c2e3b-edf8-4bc4-87bf-dadefb8b37d5",
   "metadata": {},
   "source": [
    "instead of having Date columns, for each one we split into Year, Month and Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe7bffe-1418-4305-af2c-ba9eaa4dc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN DATE\n",
    "for col in date_cols:\n",
    "    # Convert to datetime\n",
    "    train[col] = pd.to_datetime(train[col], errors='coerce')\n",
    "    test[col] = pd.to_datetime(test[col], errors='coerce')\n",
    "    \n",
    "    # Extract year, month, and day\n",
    "    train[f'{col}_Year'] = train[col].dt.year\n",
    "    train[f'{col}_Month'] = train[col].dt.month\n",
    "    train[f'{col}_Day'] = train[col].dt.day\n",
    "    \n",
    "    test[f'{col}_Year'] = test[col].dt.year\n",
    "    test[f'{col}_Month'] = test[col].dt.month\n",
    "    test[f'{col}_Day'] = test[col].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca0220-3dda-4d96-9d64-3d8a9259bca1",
   "metadata": {},
   "source": [
    "some columns were floats but should be int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0361a527-d7e5-412f-b557-a994c3cb7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN INT\n",
    "int_cols = ['Age at Injury', 'Birth Year', 'IME-4 Count', 'Number of Dependents']\n",
    "for col in int_cols:\n",
    "    train[col] = train[col].astype('Int64')\n",
    "    test[col] = test[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c28747-881a-4d06-98ab-07933335ddf2",
   "metadata": {},
   "source": [
    "create dictionaries for mapping codes to descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc622617-52ff-4e45-b188-7dad9e6b42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_maps = {\n",
    "    'Industry Code': train.dropna(subset=['Industry Code', 'Industry Code Description']).set_index('Industry Code')['Industry Code Description'].to_dict(),\n",
    "    'WCIO Cause of Injury Code': train.dropna(subset=['WCIO Cause of Injury Code', 'WCIO Cause of Injury Description']).set_index('WCIO Cause of Injury Code')['WCIO Cause of Injury Description'].to_dict(),\n",
    "    'WCIO Nature of Injury Code': train.dropna(subset=['WCIO Nature of Injury Code', 'WCIO Nature of Injury Description']).set_index('WCIO Nature of Injury Code')['WCIO Nature of Injury Description'].to_dict(),\n",
    "    'WCIO Part Of Body Code': train.dropna(subset=['WCIO Part Of Body Code', 'WCIO Part Of Body Description']).set_index('WCIO Part Of Body Code')['WCIO Part Of Body Description'].to_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b843b-1c8d-430b-bc4e-28292d2b9265",
   "metadata": {},
   "source": [
    "some columns were numeric but represented categorical data\n",
    "\n",
    "we will detailed this further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f388d1-2f41-47cd-bdb9-3a61e125a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN OBJECT\n",
    "float_to_object = ['Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "for col in float_to_object:\n",
    "    train[col] = train[col].astype('Int64')\n",
    "    test[col] = test[col].astype('Int64')\n",
    "\n",
    "def to_object(train, val, test):\n",
    "    for col in float_to_object:\n",
    "        train[col] = train[col].astype('object')\n",
    "        val[col] = val[col].astype('object')\n",
    "        test[col] = test[col].astype('object')\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176513d3-03b6-4bb5-8416-1607b143e984",
   "metadata": {},
   "source": [
    "for Zip Code we classified it as NY Resident, Non-NY US Resident and Non-US Resident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "221202ca-b76c-4c7c-a081-9988d24a1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiy them as Non-US residents\n",
    "train['Zip Code'] = train['Zip Code'].apply(\n",
    "    lambda x: x[:2] if isinstance(x, str) and len(x) == 5 and x.isdigit() else ('Non-US Resident' if pd.notna(x) else np.nan)\n",
    ")\n",
    "test['Zip Code'] = test['Zip Code'].apply(\n",
    "    lambda x: x[:2] if isinstance(x, str) and len(x) == 5 and x.isdigit() else ('Non-US Resident' if pd.notna(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8908f7-b8cb-428b-ae01-d27db6bfa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip codes that start with 1 come from NY state - where the data set is based\n",
    "# we decide to divide those that are from NY from those that even though are US residents, are not from NY\n",
    "train['Zip Code'] = np.where(\n",
    "    (train['Zip Code'] != 'Unknown') & \n",
    "    (train['Zip Code'] != 'Non-US Resident') & \n",
    "    train['Zip Code'].notna() & \n",
    "    train['Zip Code'].str.startswith('1'), \n",
    "    'NY Resident', \n",
    "    np.where(\n",
    "        (train['Zip Code'] != 'Unknown') & \n",
    "        (train['Zip Code'] != 'Non-US Resident') & \n",
    "        train['Zip Code'].notna(), \n",
    "        'Non-NY US Residents', \n",
    "        train['Zip Code']\n",
    "    )\n",
    ")\n",
    "test['Zip Code'] = np.where(\n",
    "    (test['Zip Code'] != 'Unknown') & \n",
    "    (test['Zip Code'] != 'Non-US Resident') & \n",
    "    test['Zip Code'].notna() & \n",
    "    test['Zip Code'].str.startswith('1'), \n",
    "    'NY Resident', \n",
    "    np.where(\n",
    "        (test['Zip Code'] != 'Unknown') & \n",
    "        (test['Zip Code'] != 'Non-US Resident') & \n",
    "        test['Zip Code'].notna(), \n",
    "        'Non-NY US Residents', \n",
    "        test['Zip Code']\n",
    "    )\n",
    ")\n",
    "# print(train['Zip Code'].value_counts())\n",
    "# print() \n",
    "# print('NaN:', train['Zip Code'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ebb1c-e18c-4c26-80c7-ef128b21d793",
   "metadata": {},
   "source": [
    "## Feature Engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06ed50-7420-492e-a97e-31af6d3fd71b",
   "metadata": {},
   "source": [
    "we add 4 new features representing the time space between Accident Date and other dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce397b06-a84b-4c95-a535-6f2d93f23101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date feature engineering for train\n",
    "train['Assembly_to_Accident'] = (train['Assembly Date'] - train['Accident Date']).dt.days.astype('Int64')\n",
    "train['C2_to_Accident'] = (train['C-2 Date'] - train['Accident Date']).dt.days.astype('Int64')\n",
    "train['C3_to_Accident'] = (train['C-3 Date'] - train['Accident Date']).dt.days.astype('Int64')\n",
    "train['Hearing_to_Accident'] = (train['First Hearing Date'] - train['Accident Date']).dt.days.astype('Int64')\n",
    "\n",
    "# Date feature engineering for test\n",
    "test['Assembly_to_Accident'] = (test['Assembly Date'] - test['Accident Date']).dt.days.astype('Int64')\n",
    "test['C2_to_Accident'] = (test['C-2 Date'] - test['Accident Date']).dt.days.astype('Int64')\n",
    "test['C3_to_Accident'] = (test['C-3 Date'] - test['Accident Date']).dt.days.astype('Int64')\n",
    "test['Hearing_to_Accident'] = (test['First Hearing Date'] - test['Accident Date']).dt.days.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86da56-6dec-4fb9-8d52-627b64c419cf",
   "metadata": {},
   "source": [
    "we add 4 new features representing the age in different moments related to the claim process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1638ebb6-0afa-4d9a-bb2e-69d94e38c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age-based features for train\n",
    "train['Age_at_Assembly'] = (train['Age at Injury'] + (train['Assembly Date'] - train['Accident Date']).dt.days / 365).astype('Int64')\n",
    "train['Age_at_C2'] = (train['Age at Injury'] + (train['C-2 Date'] - train['Accident Date']).dt.days / 365).astype('Int64')\n",
    "train['Age_at_C3'] = (train['Age at Injury'] + (train['C-3 Date'] - train['Accident Date']).dt.days / 365).astype('Int64')\n",
    "train['Age_at_Hearing'] = (train['Age at Injury'] + (train['First Hearing Date'] - train['Accident Date']).dt.days / 365).astype('Int64')\n",
    "\n",
    "# Age-based features for test\n",
    "test['Age_at_Assembly'] = (test['Age at Injury'] + (test['Assembly Date'] - test['Accident Date']).dt.days / 365).astype('Int64')\n",
    "test['Age_at_C2'] = (test['Age at Injury'] + (test['C-2 Date'] - test['Accident Date']).dt.days / 365).astype('Int64')\n",
    "test['Age_at_C3'] = (test['Age at Injury'] + (test['C-3 Date'] - test['Accident Date']).dt.days / 365).astype('Int64')\n",
    "test['Age_at_Hearing'] = (test['Age at Injury'] + (test['First Hearing Date'] - test['Accident Date']).dt.days / 365).astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615f46f-8982-4ebe-ba13-799ed0069878",
   "metadata": {},
   "source": [
    "we add a new column to group the wages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704eed84-eb0f-457a-a90e-4caf9b25d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary column for wages greater than 0\n",
    "positive_wages_train = train['Average Weekly Wage'] > 0\n",
    "\n",
    "# Apply qcut to positive wages only\n",
    "wage_groups_train = pd.qcut(\n",
    "    train.loc[positive_wages_train, 'Average Weekly Wage'], \n",
    "    q=10, \n",
    "    labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    ")\n",
    "\n",
    "# Create a new column for wage groups, preserving NaN values\n",
    "train['Wage Group'] = pd.Series(index=train.index)\n",
    "train.loc[positive_wages_train, 'Wage Group'] = wage_groups_train.astype(int)\n",
    "\n",
    "# Assign group 0 to wages that are exactly 0\n",
    "train.loc[train['Average Weekly Wage'] == 0, 'Wage Group'] = 0\n",
    "\n",
    "# Engenharia de recursos de grupos salariais para test\n",
    "# Create a temporary column for wages greater than 0\n",
    "positive_wages_test = test['Average Weekly Wage'] > 0\n",
    "\n",
    "# Apply qcut to positive wages only\n",
    "wage_groups_test = pd.qcut(\n",
    "    test.loc[positive_wages_test, 'Average Weekly Wage'], \n",
    "    q=10, \n",
    "    labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    ")\n",
    "\n",
    "# Create a new column for wage groups, preserving NaN values\n",
    "test['Wage Group'] = pd.Series(index=test.index)\n",
    "test.loc[positive_wages_test, 'Wage Group'] = wage_groups_test.astype(int)\n",
    "\n",
    "# Assign group 0 to wages that are exactly 0\n",
    "test.loc[test['Average Weekly Wage'] == 0, 'Wage Group'] = 0\n",
    "\n",
    "# Convert 'Wage Group' to categorical\n",
    "train['Wage Group'] = train['Wage Group'].astype('category')\n",
    "test['Wage Group'] = test['Wage Group'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e7679-c3a2-43e2-8e11-c8a48a6d51c6",
   "metadata": {},
   "source": [
    "we add a column representing the County of Injury distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9100305-bd01-4fe9-b849-ca3e095fe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = { \"SUFFOLK\": 45.4, \"QUEENS\": 8.5, \"KINGS\": 7.5, \"NASSAU\": 20.1,\n",
    "        \"BRONX\": 10.3, \"ERIE\": 371.1, \"NEW YORK\": 0, \"WESTCHESTER\": 20.5,\n",
    "        \"MONROE\": 334.8, \"ORANGE\": 59.5, \"ONONDAGA\": 194.8, \"RICHMOND\": 17.1,\n",
    "        \"ALBANY\": 155.1, \"DUTCHESS\": 76.3, \"ROCKLAND\": 30.8, \"SARATOGA\": 143.1, \n",
    "        \"NIAGARA\": 373.9, \"BROOME\": 173.1, \"ONEIDA\": 203.1, \"RENSSELAER\": 145.9, \n",
    "        \"ULSTER\": 86.3, \"CAYUGA\": 221.9, \"HERKIMER\": 213.9, \"CHAUTAUQUA\": 407.9, \n",
    "        \"ONTARIO\": 264.9, \"CHEMUNG\": 201.9, \"OSWEGO\": 243.9, \"FULTON\": 223.1, \n",
    "        \"PUTNAM\": 51.9, \"ST. LAWRENCE\": 314.9, \"JEFFERSON\": 341.1, \"CLINTON\": 304.9, \n",
    "        \"CATTARAUGUS\": 371.9, \"SULLIVAN\": 97.3, \"GENESEE\": 344.9, \"COLUMBIA\": 120.1,\n",
    "        \"MADISON\": 193.9, \"WARREN\": 194.9, \"LIVINGSTON\": 276.9, \"DELAWARE\": 137.1,\n",
    "        \"WASHINGTON\": 204.9, \"GREENE\": 124.9, \"ALLEGANY\": 346.9, \"WAYNE\": 294.9,\n",
    "        \"CHENANGO\": 181.9, \"TOMPKINS\": 209.9, \"ORLEANS\": 323.9, \"SCHENECTADY\": 156.1,\n",
    "        \"FRANKLIN\": 294.9, \"SENECA\": 234.9, \"LEWIS\": 266.9, \"TIOGA\": 187.1, \"STEUBEN\": 246.9, \n",
    "        \"ESSEX\": 214.9, \"SCHUYLER\": 206.1, \"OTSEGO\": 165.1, \"CORTLAND\": 193.9, \n",
    "        \"WYOMING\": 313.9, \"MONTGOMERY\": 173.9, \"SCHOHARIE\": 146.1, \"YATES\": 243.9,\"HAMILTON\": 221.9\n",
    "}\n",
    "\n",
    "# Create a list of distances\n",
    "distances = list(counties.values())\n",
    "\n",
    "# Calculate the mean distance\n",
    "mean_distance = statistics.mean(distances)\n",
    "\n",
    "# Add the \"UNKNOWN\" county to the dictionary with the mean distance\n",
    "counties[\"UNKNOWN\"] = mean_distance\n",
    "\n",
    "# Create a new column in the df_train DataFrame called distance_of_county\n",
    "train['distance_of_county'] = train['County of Injury'].map(counties)\n",
    "test['distance_of_county'] = test['County of Injury'].map(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d8d87-311c-42c1-8861-16e13998022b",
   "metadata": {},
   "source": [
    "dropping the original Date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5911ac0c-4cc0-4c93-98ff-11ad650cef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=date_cols, inplace=True)\n",
    "test.drop(columns=date_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb421150-fd89-4f64-83f6-4e25dc02586e",
   "metadata": {},
   "source": [
    "### Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "752b6358-c8a3-4451-8ee7-3cf7dd8667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(X_num, X_cat):\n",
    "    return pd.concat([X_num, X_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ddad604-aa35-441d-baa6-26fb85a67ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(X):\n",
    "    X_num = X.select_dtypes(include=np.number)\n",
    "    X_cat = X.select_dtypes(exclude=np.number)\n",
    "    return X_num, X_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b662063-934f-47a7-a03b-8207a5a76167",
   "metadata": {},
   "source": [
    "we split our training data into train and validation\n",
    "\n",
    "we can do this using some techniques like train_test_split or using kfold or stratifiedkfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62951b01-f400-4f1e-9e07-a79dadd05dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test, method=None):\n",
    "    splits = []\n",
    "    if method is None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.3, \n",
    "                                                random_state = 0, \n",
    "                                                stratify = y, \n",
    "                                                shuffle = True)\n",
    "        splits.append((X_train, X_val, y_train, y_val))\n",
    "    elif isinstance(method, StratifiedKFold):\n",
    "        for train_index, test_index in method.split(X, y):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "            splits.append((X_train, X_val, y_train, y_val))\n",
    "    else:\n",
    "        for train_index, test_index in method.split(X):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "            splits.append((X_train, X_val, y_train, y_val))\n",
    "\n",
    "    processed_splits = []\n",
    "    for X_train, X_val, y_train, y_val in splits:\n",
    "        X_train_num = X_train.select_dtypes(include=np.number)\n",
    "        X_val_num = X_val.select_dtypes(include=np.number)\n",
    "        X_train_cat = X_train.select_dtypes(exclude=np.number)\n",
    "        X_val_cat = X_val.select_dtypes(exclude=np.number)\n",
    "        X_test_num = test.select_dtypes(include=np.number)\n",
    "        X_test_cat = test.select_dtypes(exclude=np.number)\n",
    "        processed_splits.append((X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat, y_train, y_val))\n",
    "\n",
    "    return processed_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b329cc7-6400-4da9-828a-531ca4b73409",
   "metadata": {},
   "source": [
    "####  Replacing NaN with nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe679ef8-e523-4a04-a512-1d0075e60366",
   "metadata": {},
   "source": [
    "we used knn to fill the code columns, like 'Injury Code', with the value of the most similar claim \n",
    "\n",
    "and then accurately, using the dictionary build before, fill the associated description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e412b6fc-9ad4-4a4f-ae5f-e5cf6182a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Imputer(X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat, code_maps, float_to_object):\n",
    "    # Select only the float_to_object features for KNN imputation\n",
    "    float_to_object_features = [col for col in float_to_object if col in X_train_num.columns]\n",
    "    print(f\"Float to Object Features: {float_to_object_features}\")\n",
    "\n",
    "    X_train_num_knn = X_train_num[float_to_object_features]\n",
    "    X_val_num_knn = X_val_num[float_to_object_features]\n",
    "    X_test_num_knn = X_test_num[float_to_object_features]\n",
    "\n",
    "    # Impute missing values in float_to_object features using KNN\n",
    "    imputer = KNNImputer(n_neighbors=1).fit(X_train_num_knn)\n",
    "    train_num_imp_knn = imputer.transform(X_train_num_knn)\n",
    "    val_num_imp_knn = imputer.transform(X_val_num_knn)\n",
    "    test_num_imp_knn = imputer.transform(X_test_num_knn)\n",
    "\n",
    "    # Convert imputed numerical features back to DataFrames\n",
    "    train_num_df_knn = pd.DataFrame(train_num_imp_knn, columns=X_train_num_knn.columns, index=X_train_num_knn.index)\n",
    "    val_num_df_knn = pd.DataFrame(val_num_imp_knn, columns=X_val_num_knn.columns, index=X_val_num_knn.index)\n",
    "    test_num_df_knn = pd.DataFrame(test_num_imp_knn, columns=X_test_num_knn.columns, index=X_test_num_knn.index)\n",
    "\n",
    "    # Replace the original float_to_object features with the imputed ones\n",
    "    X_train_num[float_to_object_features] = train_num_df_knn\n",
    "    X_val_num[float_to_object_features] = val_num_df_knn\n",
    "    X_test_num[float_to_object_features] = test_num_df_knn\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    train_combined = combine_df(X_train_num, X_train_cat)\n",
    "    val_combined = combine_df(X_val_num, X_val_cat)\n",
    "    test_combined = combine_df(X_test_num, X_test_cat)\n",
    "\n",
    "    #print(train_combined.columns)\n",
    "    \n",
    "    # Map codes to descriptions\n",
    "    for code, map_dict in code_maps.items():\n",
    "        code_name = code.replace(' Code', '')\n",
    "        if f'{code_name} Description' in train_combined.columns:\n",
    "            train_combined[f'{code_name} Description'] = train_combined[code].map(map_dict).fillna(train_combined[f'{code_name} Description'])\n",
    "            val_combined[f'{code_name} Description'] = val_combined[code].map(map_dict).fillna(val_combined[f'{code_name} Description'])\n",
    "            test_combined[f'{code_name} Description'] = test_combined[code].map(map_dict).fillna(test_combined[f'{code_name} Description'])\n",
    "        else:\n",
    "            print(f\"Column {code_name} Description does not exist\")\n",
    "\n",
    "    X_train_num = train_combined.select_dtypes(include=np.number)\n",
    "    X_val_num = val_combined.select_dtypes(include=np.number)\n",
    "    X_test_num = test_combined.select_dtypes(include=np.number)\n",
    "    X_train_cat = train_combined.select_dtypes(exclude=np.number)\n",
    "    X_val_cat = val_combined.select_dtypes(exclude=np.number)\n",
    "    X_test_cat = test_combined.select_dtypes(exclude=np.number)\n",
    "\n",
    "    return X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84806a65-51f9-4153-af5c-ef4164671347",
   "metadata": {},
   "source": [
    "we decided to input the other's missing values with the median or most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "479ada7c-410d-4564-811b-2f604e0ab4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing(X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat):\n",
    "    #Using median for numerical data\n",
    "    num_imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(num_imputer.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(num_imputer.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    #Using most frequent for categorical data\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train_cat), columns=X_train_cat.columns)\n",
    "    X_val_cat = pd.DataFrame(cat_imputer.transform(X_val_cat), columns=X_val_cat.columns)\n",
    "    X_test_cat = pd.DataFrame(cat_imputer.transform(X_test_cat), columns=X_test_cat.columns)\n",
    "\n",
    "    return X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8fed3-e529-4598-b392-0017bbefcc40",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"outliers\">\n",
    "    \n",
    "## 4.4 Outliers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42fe24cf-fe52-4d36-8008-bf61122cad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnumeric_columns = train.select_dtypes(include=['number']).columns\\n\\nnum_cols = len(numeric_columns)\\ncols = 2  # Número de colunas na grade\\nrows = (num_cols // cols) + (num_cols % cols) \\n\\nplt.figure(figsize=(15, 5 * rows))\\nfor i, col in enumerate(numeric_columns, 1):\\n    plt.subplot(rows, cols, i)\\n    sns.boxplot(x=train[col])\\n    plt.title(f'Boxplot of {col}')\\n    plt.xlabel(col)\\n\\nplt.tight_layout()\\nplt.show() \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "numeric_columns = train.select_dtypes(include=['number']).columns\n",
    "\n",
    "num_cols = len(numeric_columns)\n",
    "cols = 2  # Número de colunas na grade\n",
    "rows = (num_cols // cols) + (num_cols % cols) \n",
    "\n",
    "plt.figure(figsize=(15, 5 * rows))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.boxplot(x=train[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7deb874a-8769-420e-854b-8d524ed3c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IQR and identify outliers for a specific column\n",
    "def identify_outliers_iqr_column(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "825e8da6-96d3-44db-aa14-59691407a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train):\n",
    "    outliers = pd.Series([False] * len(X_train))\n",
    "    \n",
    "    # Removing outliers for 'Age at Injury'\n",
    "    if 'Age at Injury' in X_train.columns:\n",
    "        outliers_age = (X_train['Age at Injury'] < 12) | (X_train['Age at Injury'] > 80)\n",
    "        outliers = outliers | outliers_age\n",
    "\n",
    "    # Removing outliers for 'Average Weekly Wage'\n",
    "    if 'Average Weekly Wage' in X_train.columns:\n",
    "        outliers_wage = X_train['Average Weekly Wage'] > 1e5\n",
    "        outliers = outliers | outliers_wage\n",
    "    \n",
    "    # Removing outliers for 'Birth Year'\n",
    "    if 'Birth Year' in X_train.columns:\n",
    "        outliers_birth_year = X_train['Birth Year'] == 0\n",
    "        outliers = outliers | outliers_birth_year\n",
    "    \n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fecf365-0c6d-4c4d-a090-a34708ed2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_outliers(X_train, columns_outliers): #, y_train\n",
    "    for col in columns_outliers:\n",
    "        outliers = identify_outliers_iqr_column(X_train, col)\n",
    "        # Replace outliers with median\n",
    "        median = X_train[col].median()\n",
    "        X_train.loc[outliers, col] = median\n",
    "    \n",
    "    return X_train #, y_train\n",
    "\n",
    "#train = sub_outliers(train, train.select_dtypes(include=[np.number]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22c49ef9-7e18-491d-9ee6-fed6231d62eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnumeric_columns = train.select_dtypes(include=['number']).columns\\n\\nnum_cols = len(numeric_columns)\\ncols = 2  # Número de colunas na grade\\nrows = (num_cols // cols) + (num_cols % cols)\\n\\nplt.figure(figsize=(15, 5 * rows))\\nfor i, col in enumerate(numeric_columns, 1):\\n    plt.subplot(rows, cols, i)\\n    sns.boxplot(x=train[col])\\n    plt.title(f'Boxplot of {col}')\\n    plt.xlabel(col)\\n\\nplt.tight_layout()\\nplt.show() \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "numeric_columns = train.select_dtypes(include=['number']).columns\n",
    "\n",
    "num_cols = len(numeric_columns)\n",
    "cols = 2  # Número de colunas na grade\n",
    "rows = (num_cols // cols) + (num_cols % cols)\n",
    "\n",
    "plt.figure(figsize=(15, 5 * rows))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.boxplot(x=train[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09074c-1cf9-449c-9e30-1cf79906f10e",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a44d6-dc69-4a40-be3d-f710c0ac9789",
   "metadata": {},
   "source": [
    "if var == 0 then drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a5ec16a-9f49-4907-989a-23d68d7573a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(X_train, threshold, return_variances=False):\n",
    "    variances = X_train.var()\n",
    "    low_variance_cols = variances[variances == threshold].index.tolist()\n",
    "    if return_variances:\n",
    "        return low_variance_cols, variances.to_dict()\n",
    "    return low_variance_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb55930-097b-4cb2-824a-9c3d3c13d7df",
   "metadata": {},
   "source": [
    "spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b00d5349-08c5-4136-8f9b-0f6c5aac35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_correlated_vars(X_train, threshold):\n",
    "    cor_spearman = X_train.corr(method='spearman')\n",
    "    correlated_pairs = []\n",
    "    for i in range(len(cor_spearman.columns)):\n",
    "        for j in range(i):\n",
    "            correlation = cor_spearman.iloc[i, j]\n",
    "            if abs(correlation) >= threshold:\n",
    "                correlated_pairs.append({\n",
    "                    \"feature_1\": cor_spearman.columns[i],\n",
    "                    \"feature_2\": cor_spearman.columns[j],\n",
    "                    \"correlation\": correlation\n",
    "                })\n",
    "    return correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047a24f-0639-4312-a523-acc8ec6b2df2",
   "metadata": {},
   "source": [
    "chi square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4653a600-4a2d-48d6-a436-3b43aa309bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_independence(x,y,alpha=0.05):        \n",
    "    dfObserved = pd.crosstab(y,x) \n",
    "    if dfObserved.empty:\n",
    "        print(f\"Skipping column {x.name} due to empty observed table.\")\n",
    "        return None\n",
    "    if x.nunique() <= 1:\n",
    "        print(f\"Skipping column {x.name} as it has <= 1 unique value.\")\n",
    "        return None\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
    "    is_important = p < alpha\n",
    "    result = {\n",
    "        \"feature\": x.name,\n",
    "        \"p_value\": p,\n",
    "        \"chi2_stat\": chi2,\n",
    "        \"is_important\": is_important\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e28f2f04-93d6-4448-95eb-7abaf46684fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(X_train, y, alpha=0.05):\n",
    "    if X_train.empty or y.empty:\n",
    "        raise ValueError(\"X_train or y is empty.\")\n",
    "    if len(y.unique()) < 2:\n",
    "        raise ValueError(\"y must have at least two unique classes.\")\n",
    "    results = []\n",
    "    for var in X_train.columns:\n",
    "        test_result = test_independence(X_train[var], y, alpha)\n",
    "        if test_result is None:\n",
    "            print(\"Deu none\")\n",
    "        results.append(test_result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    not_important_features = results_df[~results_df[\"is_important\"]][\"feature\"].tolist()\n",
    "    \n",
    "    return results_df, not_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c779a-a9b4-4003-808f-8d2c1d5b8b78",
   "metadata": {},
   "source": [
    "relation with the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9100ea1f-065b-4d67-a7d9-98085c7100cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_charts_categorical(df, feature, target):\n",
    "    cont_tab = pd.crosstab(df[feature], df[target], margins=True)\n",
    "    categories = cont_tab.index[:-1]\n",
    "    target_categories = cont_tab.columns[:-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    bottom = np.zeros(len(categories))\n",
    "    colors = plt.cm.tab20.colors  # Use a colormap for different colors\n",
    "    bars = []\n",
    "    for i, target_cat in enumerate(target_categories):\n",
    "        bar = plt.bar(categories, cont_tab.iloc[:-1, i].values, 0.55, bottom=bottom, color=colors[i % len(colors)])\n",
    "        bars.append(bar[0])\n",
    "        bottom += cont_tab.iloc[:-1, i].values\n",
    "    plt.legend(bars, [f'$y_i={cat}$' for cat in target_categories])\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, i].values, cont_tab.iloc[:-1, -1].values) for i in range(len(target_categories))])\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    bottom = np.zeros(len(categories))\n",
    "    bars = []\n",
    "    for i, target_cat in enumerate(target_categories):\n",
    "        bar = plt.bar(categories, obs_pct[i], 0.55, bottom=bottom, color=colors[i % len(colors)])\n",
    "        bars.append(bar[0])\n",
    "        bottom += obs_pct[i]\n",
    "    plt.legend(bars, [f'$y_i={cat}$' for cat in target_categories])\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_and_test_correlation(df, target):\n",
    "    for feature in df.select_dtypes(include='object').columns:\n",
    "        print(f\"Generating bar charts for {feature}...\")\n",
    "        bar_charts_categorical(df, feature, target)\n",
    "\n",
    "#plot_and_test_correlation(train, 'Claim Injury Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927a1f9-0a73-411d-be4a-cd5fa7736b8e",
   "metadata": {},
   "source": [
    "rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8d8aa6e-2d57-409d-ada2-c16299a54a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimal_features_rfe(X_train, y_train, X_val, y_val, model, scoring_function=None):\n",
    "    if scoring_function is None:\n",
    "        scoring_function = lambda model, X, y: model.score(X, y)\n",
    "\n",
    "    nof_list=np.arange(1, X_train.shape[1]+1)\n",
    "    high_score = 0\n",
    "    nof = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "\n",
    "        # Storing results on training data\n",
    "        train_score = scoring_function(model, X_train_rfe, y_train)\n",
    "        train_score_list.append(train_score)\n",
    "\n",
    "        # Storing results on validation data\n",
    "        val_score = scoring_function(model, X_val_rfe, y_val)\n",
    "        val_score_list.append(val_score)\n",
    "\n",
    "        # Check best score\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            nof = n\n",
    "\n",
    "    # Fit RFE with the optimal number of features\n",
    "    rfe = RFE(estimator=model, n_features_to_select=nof)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "    return selected_features, train_score_list, val_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130d3f1-abc2-42c2-ac9d-4fd4ef28925d",
   "metadata": {},
   "source": [
    "embedded methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da310080-4a95-4a36-8153-d2482dc84ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features_embedded(X_train, y_train, model, threshold=None):\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the coefficients or feature importances\n",
    "    if hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            coef = pd.Series(model.coef_.mean(axis=0), index=X_train.columns)\n",
    "        else:\n",
    "            coef = pd.Series(model.coef_, index=X_train.columns)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        coef = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    else:\n",
    "        raise ValueError(\"The model does not have coef_ or feature_importances_ attributes\")\n",
    "    \n",
    "    if threshold is not None:\n",
    "        selected_features = coef[coef.abs() > threshold].index.tolist()\n",
    "    else:\n",
    "        selected_features = coef[coef != 0].index.tolist()\n",
    "    \n",
    "    return selected_features, coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87424e-8524-4ce2-ad5b-e1dba3647c67",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d479f7-a4f3-43d2-8590-23f5708bc377",
   "metadata": {},
   "source": [
    "we scaled our data to improve model performance and to equal the features contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e60bb83-96fb-424e-b12e-3895b9a91c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X_train, X_val, X_test, scaler):\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns).set_index(X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns).set_index(X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns).set_index(X_test.index)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48e698-ec33-4f64-b7c5-4fdc2707a726",
   "metadata": {},
   "source": [
    "### Reducing Cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb58e37-d66d-4e1b-9a8a-b6383b4b3e36",
   "metadata": {},
   "source": [
    "to use some encoding strategies like OneHotEnconding we change some minority classes in each column as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff67f63a-a743-4e5d-9f2e-b04cc9b5158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cardinality(df, threshold=10, other_label='Other'):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        value_counts = df[col].value_counts()\n",
    "        frequent_values = value_counts[value_counts > threshold].index\n",
    "        df[col] = df[col].apply(lambda x: x if x in frequent_values else other_label)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15eb733-4db6-4c73-955c-db32183e4e5b",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28935278-edd0-407c-948c-23608207f884",
   "metadata": {},
   "source": [
    "we use encoding to prepare our categorical features to be used by a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3658b2-82a4-4940-b4cb-cfc99d7fe5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_independent(X_train, X_val, X_test, encoder):\n",
    "    X_train = X_train.astype(str)\n",
    "    X_val = X_val.astype(str)\n",
    "    X_test = X_test.astype(str)\n",
    "    \n",
    "    encoder.fit(X_train)\n",
    "    X_train_encoded = encoder.transform(X_train) \n",
    "    X_val_encoded = encoder.transform(X_val)\n",
    "    X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "    if isinstance(encoder, OneHotEncoder):\n",
    "        feature_names = encoder.get_feature_names_out(X_train.columns)\n",
    "        X_train_encoded = pd.DataFrame(X_train_encoded, columns=feature_names, index=X_train.index)\n",
    "        X_val_encoded = pd.DataFrame(X_val_encoded, columns=feature_names, index=X_val.index)\n",
    "        X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names, index=X_test.index)\n",
    "    else:\n",
    "        X_train_encoded = pd.DataFrame(X_train_encoded, columns=X_train.columns, index=X_train.index)\n",
    "        X_val_encoded = pd.DataFrame(X_val_encoded, columns=X_val.columns, index=X_val.index)\n",
    "        X_test_encoded = pd.DataFrame(X_test_encoded, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    return X_train_encoded, X_val_encoded, X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac5822d9-c10e-46dd-8adc-a416bd2786a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_dependent(y_train, y_val, encoder):\n",
    "    encoder.fit(y_train)\n",
    "    y_train_encoded = pd.Series(encoder.transform(y_train))\n",
    "    y_val_encoded = pd.Series(encoder.transform(y_val))\n",
    "\n",
    "    return y_train_encoded, y_val_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc536f-eed9-484b-af35-ebceb5071be8",
   "metadata": {},
   "source": [
    "### Balancing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb156fe2-2be9-4c0f-8873-07297c86870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sampling_strategy(y):\n",
    "    class_counts = np.bincount(y)\n",
    "    max_count = np.max(class_counts)\n",
    "    sampling_strategy = {}\n",
    "    for i, count in enumerate(class_counts):\n",
    "        if i in [5, 6, 7]:  # minority classes\n",
    "            sampling_strategy[i] = 4000\n",
    "        else:\n",
    "            sampling_strategy[i] = count  # use the original count for other classes\n",
    "    return sampling_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94cc52d8-f19c-465d-a048-ca7a30ec5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X, y, method='oversample'):\n",
    "    if method == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=42)\n",
    "    elif method == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "    elif method == 'smote':\n",
    "        sampler = SMOTEENN(random_state=42, sampling_strategy=custom_sampling_strategy(y))\n",
    "    else:\n",
    "        raise ValueError(\"Method should be 'oversample', 'undersample', or 'smote'\")\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38ef55-da04-4203-8550-c26756db7dd0",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f14e5934-5651-4054-9ed9-62e956bcf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X_train, X_val, X_test, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    pca_feat_names = [f'PC{i}' for i in range(n_components)]\n",
    "\n",
    "    X_train_pca = pd.DataFrame(X_train_pca, index=X_train.index, columns=pca_feat_names)\n",
    "    X_val_pca = pd.DataFrame(X_val_pca, index=X_val.index, columns=pca_feat_names)\n",
    "    X_test_pca = pd.DataFrame(X_test_pca, index=X_test.index, columns=pca_feat_names)\n",
    "    \n",
    "    return X_train_pca, X_val_pca, X_test_pca, pca_feat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8801700-dbe3-41c8-b1df-279c5cf52019",
   "metadata": {},
   "source": [
    "## Modelling and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b299f872-b810-45d4-acab-87a5c26480e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipeline(train, test, split_method, feature_selection, scaler, encoder_independent, encoder_dependent, balance_method):\n",
    "    X = train.drop(columns=['Agreement Reached'])\n",
    "    y = train['Agreement Reached']\n",
    "    print(\"Starting Split Data\")\n",
    "    splits = split_data(X, y, test, split_method)\n",
    "    print(\"Split data OK\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat, y_train, y_val in splits:\n",
    "        print(\"Starting Outliers\")\n",
    "        print(f\"Train shape before removing outliers: {X_train_num.shape}\")\n",
    "        outliers = remove_outliers(X_train_num)\n",
    "        X_train_num = X_train_num[~outliers]\n",
    "        X_train_cat = X_train_cat[~outliers]\n",
    "        y_train = y_train[~outliers]\n",
    "        print(f\"Train shape after removing outliers: {X_train_num.shape}\")\n",
    "        print(\"Outliers OK\")\n",
    "\n",
    "        # print(\"Starting KNN Imputer\")\n",
    "        # X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat = KNN_Imputer(X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat, code_maps, float_to_object)\n",
    "        # print(\"KNN Imputer OK\")\n",
    "        print(\"Starting Imputing\")\n",
    "        X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat = imputing(X_train_num, X_val_num, X_test_num, X_train_cat, X_val_cat, X_test_cat)\n",
    "        print(\"Imputing OK\")\n",
    "\n",
    "        X_train = combine_df(X_train_num, X_train_cat)\n",
    "        X_val = combine_df(X_val_num, X_val_cat)\n",
    "        X_test = combine_df(X_test_num, X_test_cat)\n",
    "        X_train, X_val, X_test = to_object(X_train, X_val, X_test)\n",
    "        X_train_num, X_train_cat = split_df(X_train)\n",
    "        X_val_num, X_val_cat = split_df(X_val)\n",
    "        X_test_num, X_test_cat = split_df(X_test)\n",
    "\n",
    "        if feature_selection:\n",
    "            print(\"Starting Variance\")\n",
    "            low_variance_cols, variances = variance(X_train_num, 0, True)\n",
    "            print(low_variance_cols)\n",
    "            print(variances)\n",
    "            print(\"Variance OK\")\n",
    "            print(\"Starting High Correlated Vars\")\n",
    "            X_corr = X_train_num.copy()\n",
    "            X_corr['Claim Injury Type'] = y_train\n",
    "            correlated_pairs = high_correlated_vars(X_corr, 0.8)\n",
    "            print(correlated_pairs)\n",
    "            print(\"High Correlated Vars OK\")\n",
    "            # print(\"Starting Chi Square\")\n",
    "            # results, not_important_fatures = chi_square(X_train_cat, y_train)\n",
    "            # print(results)\n",
    "            # print(not_important_fatures)\n",
    "            # print(\"Chi Square OK\")\n",
    "\n",
    "        print(\"Starting Scaling\")\n",
    "        X_train_num, X_val_num, X_test_num = scaling(X_train_num, X_val_num, X_test_num, scaler)\n",
    "        print(\"Scaling OK\")\n",
    "\n",
    "        print(\"Reducing Cardinality\")\n",
    "        X_train_cat = reduce_cardinality(X_train_cat)\n",
    "        X_val_cat = reduce_cardinality(X_val_cat)\n",
    "        X_test_cat = reduce_cardinality(X_test_cat)\n",
    "        print(\"Reducing Cardinality OK\")\n",
    "\n",
    "        print(\"Starting Encoding Independent\")\n",
    "        X_train_cat, X_val_cat, X_test_cat = encoding_independent(X_train_cat, X_val_cat, X_test_cat, encoder_independent)\n",
    "        print(\"Encoding Independent OK\")\n",
    "        print(\"Starting Encoding Dependent\")\n",
    "        y_train, y_val = encoding_dependent(y_train, y_val, encoder_dependent)\n",
    "        print(\"Encoding Dependent OK\")\n",
    "\n",
    "        # print(\"Starting PCA\")\n",
    "        # X_train_num, X_val_num, X_test_num, pca_feat_names = apply_pca(X_train_num, X_val_num, X_test_num, 10)\n",
    "        # print(\"PCA OK\")\n",
    "\n",
    "        X_train = combine_df(X_train_num, X_train_cat)\n",
    "        X_val = combine_df(X_val_num, X_val_cat)\n",
    "        X_test = combine_df(X_test_num, X_test_cat)\n",
    "        \n",
    "        if balance_method is not None:\n",
    "            print(\"Starting Balance Data\")\n",
    "            X_train, y_train = balance_data(X_train, y_train, balance_method)\n",
    "            print(\"Balance Data OK\")\n",
    "\n",
    "        results.append((X_train, X_val, X_test, y_train, y_val))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60be3cc3-ed21-4e1e-97c1-f0be126d48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_split = None\n",
    "kf = KFold(n_splits=10) #if the splits are too many, poor efficiency\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=2)  \n",
    "#loo = LeaveOneOut() not good due the size of the dataset\n",
    "skf = StratifiedKFold(n_splits=10)  #good for imbalanced datasets\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "min_max2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "standard = StandardScaler()\n",
    "robust = RobustScaler()\n",
    "\n",
    "oneHot = OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown='ignore')\n",
    "ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "label = LabelEncoder()\n",
    "\n",
    "oversample = 'oversample'\n",
    "undersample = 'undersample'\n",
    "smote = 'smote'\n",
    "no_balance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41ff4909-febe-445f-b720-6c3a8ba0644e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Split Data\n",
      "Split data OK\n",
      "Starting Outliers\n",
      "Train shape before removing outliers: (401817, 33)\n",
      "Train shape after removing outliers: (384193, 33)\n",
      "Outliers OK\n",
      "Starting Imputing\n",
      "Imputing OK\n",
      "Starting Variance\n",
      "[]\n",
      "{'Age at Injury': 198.19349163925702, 'Average Weekly Wage': 603570.7186536161, 'Birth Year': 181.3012968175019, 'IME-4 Count': 2.1396326359456688, 'Number of Dependents': 4.008093273175224, 'Accident Date_Year': 2.9666472258224332, 'Accident Date_Month': 11.881004064470508, 'Accident Date_Day': 76.65125835479293, 'Assembly Date_Year': 0.6561729948007808, 'Assembly Date_Month': 11.805455127127571, 'Assembly Date_Day': 76.25989724674965, 'C-2 Date_Year': 1.1145960289107557, 'C-2 Date_Month': 11.560124561431149, 'C-2 Date_Day': 74.0696397247857, 'C-3 Date_Year': 0.2579330823924584, 'C-3 Date_Month': 3.892666213514948, 'C-3 Date_Day': 24.953098575164642, 'First Hearing Date_Year': 0.3093508386964043, 'First Hearing Date_Month': 3.2785115143646357, 'First Hearing Date_Day': 20.097536569268026, 'Assembly_to_Accident': 299199.1515271103, 'C2_to_Accident': 205940.12568422421, 'C3_to_Accident': 47288.45082552502, 'Hearing_to_Accident': 47454.456480087574, 'Age_at_Assembly': 188.55284403241836, 'Age_at_C2': 184.59699494289234, 'Age_at_C3': 52.579403158921444, 'Age_at_Hearing': 43.475173452561826, 'distance_of_county': 15816.693168893285}\n",
      "Variance OK\n",
      "Starting High Correlated Vars\n",
      "[{'feature_1': 'Birth Year', 'feature_2': 'Age at Injury', 'correlation': -0.9431754464009562}, {'feature_1': 'Assembly Date_Year', 'feature_2': 'Accident Date_Year', 'correlation': 0.9321426106920425}, {'feature_1': 'C-2 Date_Year', 'feature_2': 'Accident Date_Year', 'correlation': 0.9167404378561256}, {'feature_1': 'C-2 Date_Year', 'feature_2': 'Assembly Date_Year', 'correlation': 0.9690170890176006}, {'feature_1': 'C-2 Date_Month', 'feature_2': 'Assembly Date_Month', 'correlation': 0.9313815007846914}, {'feature_1': 'C-2 Date_Day', 'feature_2': 'Assembly Date_Day', 'correlation': 0.8209247213863791}, {'feature_1': 'C2_to_Accident', 'feature_2': 'Assembly_to_Accident', 'correlation': 0.9490144320325645}, {'feature_1': 'Age_at_Assembly', 'feature_2': 'Age at Injury', 'correlation': 0.9859110642797486}, {'feature_1': 'Age_at_Assembly', 'feature_2': 'Birth Year', 'correlation': -0.965014546958388}, {'feature_1': 'Age_at_C2', 'feature_2': 'Age at Injury', 'correlation': 0.9779202345204536}, {'feature_1': 'Age_at_C2', 'feature_2': 'Birth Year', 'correlation': -0.955168567511003}, {'feature_1': 'Age_at_C2', 'feature_2': 'Age_at_Assembly', 'correlation': 0.9889736621549996}]\n",
      "High Correlated Vars OK\n",
      "Starting Scaling\n",
      "Scaling OK\n",
      "Reducing Cardinality\n",
      "Reducing Cardinality OK\n",
      "Starting Encoding Independent\n",
      "Encoding Independent OK\n",
      "Starting Encoding Dependent\n",
      "Encoding Dependent OK\n",
      "Starting Balance Data\n",
      "Balance Data OK\n",
      "Split 1:\n",
      "X_train shape: (337038, 48), y_train shape: (337038,)\n",
      "X_val shape: (172208, 48), y_val shape: (172208,)\n",
      "X_test shape: (387975, 48)\n"
     ]
    }
   ],
   "source": [
    "processed_data = pre_processing_pipeline(train, test, normal_split, True, standard, ordinal, label, smote)\n",
    "for i, (X_train, X_val, X_test, y_train, y_val) in enumerate(processed_data):\n",
    "    print(f\"Split {i+1}:\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403b5b5-209c-4d0b-8e7e-223a47788ea8",
   "metadata": {},
   "source": [
    "Starting Variance\n",
    "[]\n",
    "{'Age at Injury': 198.18715003163175, 'Average Weekly Wage': 593328.9346257411, 'Birth Year': 181.25345368680772, 'IME-4 Count': 2.1339682310632573, 'Agreement Reached': 0.043480791026601726, 'Number of Dependents': 4.0094608120277035, 'Accident Date_Year': 2.976560266497379, 'Accident Date_Month': 11.873280144612893, 'Accident Date_Day': 76.74232654089047, 'Assembly Date_Year': 0.6562224916754912, 'Assembly Date_Month': 11.79377792451315, 'Assembly Date_Day': 76.28770527877323, 'C-2 Date_Year': 1.116449967455882, 'C-2 Date_Month': 11.54371715451866, 'C-2 Date_Day': 74.08105468085947, 'C-3 Date_Year': 0.2574103605283446, 'C-3 Date_Month': 3.901680673864706, 'C-3 Date_Day': 25.12037227072812, 'First Hearing Date_Year': 0.309690685722038, 'First Hearing Date_Month': 3.2766458013681565, 'First Hearing Date_Day': 20.165888611257678, 'Assembly_to_Accident': 300998.0811316094, 'C2_to_Accident': 208948.8283443356, 'C3_to_Accident': 48381.570658223296, 'Hearing_to_Accident': 49116.99307839868, 'Age_at_Assembly': 188.55201250464148, 'Age_at_C2': 184.63142115116767, 'Age_at_C3': 52.75393405972783, 'Age_at_Hearing': 43.566149613971774, 'distance_of_county': 15848.565889636728}\n",
    "Variance OK\n",
    "\n",
    "Starting High Correlated Vars\n",
    "[{'feature_1': 'Birth Year', 'feature_2': 'Age at Injury', 'correlation': -0.9434682621618036}, {'feature_1': 'Assembly Date_Year', 'feature_2': 'Accident Date_Year', 'correlation': 0.9318466133057587}, {'feature_1': 'C-2 Date_Year', 'feature_2': 'Accident Date_Year', 'correlation': 0.9163718626926118}, {'feature_1': 'C-2 Date_Year', 'feature_2': 'Assembly Date_Year', 'correlation': 0.9688395211428722}, {'feature_1': 'C-2 Date_Month', 'feature_2': 'Assembly Date_Month', 'correlation': 0.9310369705137715}, {'feature_1': 'C-2 Date_Day', 'feature_2': 'Assembly Date_Day', 'correlation': 0.81956478383352}, {'feature_1': 'C2_to_Accident', 'feature_2': 'Assembly_to_Accident', 'correlation': 0.948701505976799}, {'feature_1': 'Age_at_Assembly', 'feature_2': 'Age at Injury', 'correlation': 0.9858454839258293}, {'feature_1': 'Age_at_Assembly', 'feature_2': 'Birth Year', 'correlation': -0.9654773357817418}, {'feature_1': 'Age_at_C2', 'feature_2': 'Age at Injury', 'correlation': 0.9778167646555994}, {'feature_1': 'Age_at_C2', 'feature_2': 'Birth Year', 'correlation': -0.955490072529208}, {'feature_1': 'Age_at_C2', 'feature_2': 'Age_at_Assembly', 'correlation': 0.9889200990269661}]\n",
    "High Correlated Vars OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e95ce-703d-4893-a229-b54d4bf019aa",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5a621c3-5a72-467a-8ea4-89983475c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val = processed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1a552-7d04-405b-b58a-5840519a1768",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdbabcef-34cd-4992-89ab-d999bab43a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGB model...\n",
      "Starting training with 48 features...\n",
      "[0]\tvalidation_0-logloss:0.11663\tvalidation_0-error:0.00124\n",
      "[1]\tvalidation_0-logloss:0.10487\tvalidation_0-error:0.00124\n",
      "[2]\tvalidation_0-logloss:0.09442\tvalidation_0-error:0.00124\n",
      "[3]\tvalidation_0-logloss:0.08509\tvalidation_0-error:0.00124\n",
      "[4]\tvalidation_0-logloss:0.07675\tvalidation_0-error:0.00079\n",
      "[5]\tvalidation_0-logloss:0.06928\tvalidation_0-error:0.00062\n",
      "[6]\tvalidation_0-logloss:0.06257\tvalidation_0-error:0.00049\n",
      "[7]\tvalidation_0-logloss:0.05650\tvalidation_0-error:0.00045\n",
      "[8]\tvalidation_0-logloss:0.05106\tvalidation_0-error:0.00042\n",
      "[9]\tvalidation_0-logloss:0.04617\tvalidation_0-error:0.00042\n",
      "[10]\tvalidation_0-logloss:0.04177\tvalidation_0-error:0.00038\n",
      "[11]\tvalidation_0-logloss:0.03780\tvalidation_0-error:0.00037\n",
      "[12]\tvalidation_0-logloss:0.03422\tvalidation_0-error:0.00036\n",
      "[13]\tvalidation_0-logloss:0.03101\tvalidation_0-error:0.00032\n",
      "[14]\tvalidation_0-logloss:0.02811\tvalidation_0-error:0.00028\n",
      "[15]\tvalidation_0-logloss:0.02550\tvalidation_0-error:0.00027\n",
      "[16]\tvalidation_0-logloss:0.02315\tvalidation_0-error:0.00026\n",
      "[17]\tvalidation_0-logloss:0.02101\tvalidation_0-error:0.00025\n",
      "[18]\tvalidation_0-logloss:0.01909\tvalidation_0-error:0.00024\n",
      "[19]\tvalidation_0-logloss:0.01735\tvalidation_0-error:0.00024\n",
      "[20]\tvalidation_0-logloss:0.01578\tvalidation_0-error:0.00023\n",
      "[21]\tvalidation_0-logloss:0.01436\tvalidation_0-error:0.00024\n",
      "[22]\tvalidation_0-logloss:0.01308\tvalidation_0-error:0.00023\n",
      "[23]\tvalidation_0-logloss:0.01192\tvalidation_0-error:0.00022\n",
      "[24]\tvalidation_0-logloss:0.01088\tvalidation_0-error:0.00021\n",
      "[25]\tvalidation_0-logloss:0.00993\tvalidation_0-error:0.00021\n",
      "[26]\tvalidation_0-logloss:0.00907\tvalidation_0-error:0.00020\n",
      "[27]\tvalidation_0-logloss:0.00829\tvalidation_0-error:0.00020\n",
      "[28]\tvalidation_0-logloss:0.00758\tvalidation_0-error:0.00020\n",
      "[29]\tvalidation_0-logloss:0.00695\tvalidation_0-error:0.00020\n",
      "[30]\tvalidation_0-logloss:0.00637\tvalidation_0-error:0.00019\n",
      "[31]\tvalidation_0-logloss:0.00585\tvalidation_0-error:0.00017\n",
      "[32]\tvalidation_0-logloss:0.00537\tvalidation_0-error:0.00017\n",
      "[33]\tvalidation_0-logloss:0.00494\tvalidation_0-error:0.00017\n",
      "[34]\tvalidation_0-logloss:0.00455\tvalidation_0-error:0.00017\n",
      "[35]\tvalidation_0-logloss:0.00420\tvalidation_0-error:0.00016\n",
      "[36]\tvalidation_0-logloss:0.00388\tvalidation_0-error:0.00016\n",
      "[37]\tvalidation_0-logloss:0.00358\tvalidation_0-error:0.00016\n",
      "[38]\tvalidation_0-logloss:0.00331\tvalidation_0-error:0.00016\n",
      "[39]\tvalidation_0-logloss:0.00306\tvalidation_0-error:0.00016\n",
      "[40]\tvalidation_0-logloss:0.00284\tvalidation_0-error:0.00015\n",
      "[41]\tvalidation_0-logloss:0.00264\tvalidation_0-error:0.00015\n",
      "[42]\tvalidation_0-logloss:0.00245\tvalidation_0-error:0.00015\n",
      "[43]\tvalidation_0-logloss:0.00229\tvalidation_0-error:0.00015\n",
      "[44]\tvalidation_0-logloss:0.00213\tvalidation_0-error:0.00015\n",
      "[45]\tvalidation_0-logloss:0.00200\tvalidation_0-error:0.00015\n",
      "[46]\tvalidation_0-logloss:0.00187\tvalidation_0-error:0.00015\n",
      "[47]\tvalidation_0-logloss:0.00175\tvalidation_0-error:0.00014\n",
      "[48]\tvalidation_0-logloss:0.00164\tvalidation_0-error:0.00014\n",
      "[49]\tvalidation_0-logloss:0.00155\tvalidation_0-error:0.00014\n",
      "[50]\tvalidation_0-logloss:0.00146\tvalidation_0-error:0.00014\n",
      "[51]\tvalidation_0-logloss:0.00138\tvalidation_0-error:0.00014\n",
      "[52]\tvalidation_0-logloss:0.00131\tvalidation_0-error:0.00014\n",
      "[53]\tvalidation_0-logloss:0.00124\tvalidation_0-error:0.00014\n",
      "[54]\tvalidation_0-logloss:0.00117\tvalidation_0-error:0.00014\n",
      "[55]\tvalidation_0-logloss:0.00111\tvalidation_0-error:0.00013\n",
      "[56]\tvalidation_0-logloss:0.00106\tvalidation_0-error:0.00013\n",
      "[57]\tvalidation_0-logloss:0.00101\tvalidation_0-error:0.00013\n",
      "[58]\tvalidation_0-logloss:0.00096\tvalidation_0-error:0.00013\n",
      "[59]\tvalidation_0-logloss:0.00091\tvalidation_0-error:0.00013\n",
      "[60]\tvalidation_0-logloss:0.00087\tvalidation_0-error:0.00013\n",
      "[61]\tvalidation_0-logloss:0.00083\tvalidation_0-error:0.00013\n",
      "[62]\tvalidation_0-logloss:0.00080\tvalidation_0-error:0.00013\n",
      "[63]\tvalidation_0-logloss:0.00077\tvalidation_0-error:0.00013\n",
      "[64]\tvalidation_0-logloss:0.00074\tvalidation_0-error:0.00013\n",
      "[65]\tvalidation_0-logloss:0.00072\tvalidation_0-error:0.00013\n",
      "[66]\tvalidation_0-logloss:0.00069\tvalidation_0-error:0.00013\n",
      "[67]\tvalidation_0-logloss:0.00067\tvalidation_0-error:0.00013\n",
      "[68]\tvalidation_0-logloss:0.00064\tvalidation_0-error:0.00012\n",
      "[69]\tvalidation_0-logloss:0.00062\tvalidation_0-error:0.00012\n",
      "[70]\tvalidation_0-logloss:0.00060\tvalidation_0-error:0.00012\n",
      "[71]\tvalidation_0-logloss:0.00059\tvalidation_0-error:0.00012\n",
      "[72]\tvalidation_0-logloss:0.00057\tvalidation_0-error:0.00012\n",
      "[73]\tvalidation_0-logloss:0.00055\tvalidation_0-error:0.00012\n",
      "[74]\tvalidation_0-logloss:0.00053\tvalidation_0-error:0.00012\n",
      "[75]\tvalidation_0-logloss:0.00052\tvalidation_0-error:0.00012\n",
      "[76]\tvalidation_0-logloss:0.00050\tvalidation_0-error:0.00012\n",
      "[77]\tvalidation_0-logloss:0.00049\tvalidation_0-error:0.00012\n",
      "[78]\tvalidation_0-logloss:0.00047\tvalidation_0-error:0.00012\n",
      "[79]\tvalidation_0-logloss:0.00046\tvalidation_0-error:0.00012\n",
      "[80]\tvalidation_0-logloss:0.00045\tvalidation_0-error:0.00012\n",
      "[81]\tvalidation_0-logloss:0.00044\tvalidation_0-error:0.00012\n",
      "[82]\tvalidation_0-logloss:0.00042\tvalidation_0-error:0.00012\n",
      "[83]\tvalidation_0-logloss:0.00041\tvalidation_0-error:0.00011\n",
      "[84]\tvalidation_0-logloss:0.00040\tvalidation_0-error:0.00011\n",
      "[85]\tvalidation_0-logloss:0.00039\tvalidation_0-error:0.00011\n",
      "[86]\tvalidation_0-logloss:0.00039\tvalidation_0-error:0.00011\n",
      "[87]\tvalidation_0-logloss:0.00038\tvalidation_0-error:0.00010\n",
      "[88]\tvalidation_0-logloss:0.00037\tvalidation_0-error:0.00010\n",
      "[89]\tvalidation_0-logloss:0.00036\tvalidation_0-error:0.00010\n",
      "[90]\tvalidation_0-logloss:0.00035\tvalidation_0-error:0.00010\n",
      "[91]\tvalidation_0-logloss:0.00034\tvalidation_0-error:0.00010\n",
      "[92]\tvalidation_0-logloss:0.00033\tvalidation_0-error:0.00010\n",
      "[93]\tvalidation_0-logloss:0.00032\tvalidation_0-error:0.00009\n",
      "[94]\tvalidation_0-logloss:0.00031\tvalidation_0-error:0.00009\n",
      "[95]\tvalidation_0-logloss:0.00031\tvalidation_0-error:0.00009\n",
      "[96]\tvalidation_0-logloss:0.00030\tvalidation_0-error:0.00008\n",
      "[97]\tvalidation_0-logloss:0.00029\tvalidation_0-error:0.00008\n",
      "[98]\tvalidation_0-logloss:0.00028\tvalidation_0-error:0.00007\n",
      "[99]\tvalidation_0-logloss:0.00027\tvalidation_0-error:0.00007\n",
      "[100]\tvalidation_0-logloss:0.00027\tvalidation_0-error:0.00007\n",
      "[101]\tvalidation_0-logloss:0.00027\tvalidation_0-error:0.00007\n",
      "[102]\tvalidation_0-logloss:0.00026\tvalidation_0-error:0.00007\n",
      "[103]\tvalidation_0-logloss:0.00026\tvalidation_0-error:0.00007\n",
      "[104]\tvalidation_0-logloss:0.00025\tvalidation_0-error:0.00007\n",
      "[105]\tvalidation_0-logloss:0.00025\tvalidation_0-error:0.00007\n",
      "[106]\tvalidation_0-logloss:0.00024\tvalidation_0-error:0.00006\n",
      "[107]\tvalidation_0-logloss:0.00024\tvalidation_0-error:0.00006\n",
      "[108]\tvalidation_0-logloss:0.00023\tvalidation_0-error:0.00006\n",
      "[109]\tvalidation_0-logloss:0.00023\tvalidation_0-error:0.00006\n",
      "[110]\tvalidation_0-logloss:0.00022\tvalidation_0-error:0.00006\n",
      "[111]\tvalidation_0-logloss:0.00022\tvalidation_0-error:0.00005\n",
      "[112]\tvalidation_0-logloss:0.00022\tvalidation_0-error:0.00005\n",
      "[113]\tvalidation_0-logloss:0.00021\tvalidation_0-error:0.00005\n",
      "[114]\tvalidation_0-logloss:0.00021\tvalidation_0-error:0.00005\n",
      "[115]\tvalidation_0-logloss:0.00021\tvalidation_0-error:0.00005\n",
      "[116]\tvalidation_0-logloss:0.00020\tvalidation_0-error:0.00005\n",
      "[117]\tvalidation_0-logloss:0.00020\tvalidation_0-error:0.00004\n",
      "[118]\tvalidation_0-logloss:0.00019\tvalidation_0-error:0.00004\n",
      "[119]\tvalidation_0-logloss:0.00019\tvalidation_0-error:0.00004\n",
      "[120]\tvalidation_0-logloss:0.00019\tvalidation_0-error:0.00004\n",
      "[121]\tvalidation_0-logloss:0.00018\tvalidation_0-error:0.00004\n",
      "[122]\tvalidation_0-logloss:0.00018\tvalidation_0-error:0.00004\n",
      "[123]\tvalidation_0-logloss:0.00018\tvalidation_0-error:0.00004\n",
      "[124]\tvalidation_0-logloss:0.00017\tvalidation_0-error:0.00003\n",
      "[125]\tvalidation_0-logloss:0.00017\tvalidation_0-error:0.00003\n",
      "[126]\tvalidation_0-logloss:0.00017\tvalidation_0-error:0.00003\n",
      "[127]\tvalidation_0-logloss:0.00017\tvalidation_0-error:0.00003\n",
      "[128]\tvalidation_0-logloss:0.00016\tvalidation_0-error:0.00003\n",
      "[129]\tvalidation_0-logloss:0.00016\tvalidation_0-error:0.00003\n",
      "[130]\tvalidation_0-logloss:0.00016\tvalidation_0-error:0.00003\n",
      "[131]\tvalidation_0-logloss:0.00016\tvalidation_0-error:0.00003\n",
      "[132]\tvalidation_0-logloss:0.00015\tvalidation_0-error:0.00002\n",
      "[133]\tvalidation_0-logloss:0.00015\tvalidation_0-error:0.00002\n",
      "[134]\tvalidation_0-logloss:0.00015\tvalidation_0-error:0.00002\n",
      "[135]\tvalidation_0-logloss:0.00015\tvalidation_0-error:0.00002\n",
      "[136]\tvalidation_0-logloss:0.00014\tvalidation_0-error:0.00002\n",
      "[137]\tvalidation_0-logloss:0.00014\tvalidation_0-error:0.00001\n",
      "[138]\tvalidation_0-logloss:0.00014\tvalidation_0-error:0.00001\n",
      "[139]\tvalidation_0-logloss:0.00014\tvalidation_0-error:0.00001\n",
      "[140]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[141]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[142]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[143]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[144]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[145]\tvalidation_0-logloss:0.00013\tvalidation_0-error:0.00001\n",
      "[146]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00001\n",
      "[147]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00001\n",
      "[148]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00001\n",
      "[149]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00001\n",
      "[150]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00000\n",
      "[151]\tvalidation_0-logloss:0.00012\tvalidation_0-error:0.00000\n",
      "[152]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[153]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[154]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[155]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[156]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[157]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[158]\tvalidation_0-logloss:0.00011\tvalidation_0-error:0.00000\n",
      "[159]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[160]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[161]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[162]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[163]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[164]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[165]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[166]\tvalidation_0-logloss:0.00010\tvalidation_0-error:0.00000\n",
      "[167]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[168]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[169]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[170]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[171]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[172]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[173]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[174]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[175]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[176]\tvalidation_0-logloss:0.00009\tvalidation_0-error:0.00000\n",
      "[177]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[178]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[179]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[180]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[181]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[182]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[183]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[184]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[185]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[186]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[187]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[188]\tvalidation_0-logloss:0.00008\tvalidation_0-error:0.00000\n",
      "[189]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[190]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[191]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[192]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[193]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[194]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[195]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[196]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[197]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[198]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[199]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[200]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[201]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[202]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[203]\tvalidation_0-logloss:0.00007\tvalidation_0-error:0.00000\n",
      "[204]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[205]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[206]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[207]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[208]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[209]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[210]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[211]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[212]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[213]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[214]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[215]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[216]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[217]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[218]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[219]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[220]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[221]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[222]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[223]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[224]\tvalidation_0-logloss:0.00006\tvalidation_0-error:0.00000\n",
      "[225]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[226]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[227]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[228]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[229]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[230]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[231]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[232]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[233]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[234]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[235]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[236]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[237]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[238]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[239]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[240]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[241]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[242]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[243]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[244]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[245]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[246]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[247]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[248]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n",
      "[249]\tvalidation_0-logloss:0.00005\tvalidation_0-error:0.00000\n"
     ]
    }
   ],
   "source": [
    "def train_xgb(X_train, y_train, X_val, y_val, rfe, random_state=42):\n",
    "    \"\"\"Train XGB model\"\"\"\n",
    "    print(\"\\nTraining XGB model...\")\n",
    "    print(f\"Starting training with {X_train.shape[1]} features...\")\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=random_state,\n",
    "        n_jobs=2,\n",
    "        tree_method='hist',\n",
    "        enable_categorical=True,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric=['logloss', 'error'],\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    if rfe:\n",
    "        # Select optimal features using RFE\n",
    "        selected_features, train_scores, val_scores = select_optimal_features_rfe(X_train, y_train, X_val, y_val, model)\n",
    "        print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "        print(f\"Train scores: {train_scores}\")\n",
    "        print(f\"Validation scores: {val_scores}\")\n",
    "\n",
    "        # Select best features using embedded method\n",
    "        select_features, coef = select_best_features_embedded(X_train, y_train, model, 0.01)\n",
    "        print(f\"Selected {len(select_features)} features: {select_features}\")\n",
    "        print(f\"Feature importances: {coef}\")\n",
    "    else:\n",
    "        # Train with early stopping\n",
    "        eval_set = [(X_train, y_train)]\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=eval_set,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "modelXGB = train_xgb(X_train, y_train, X_val, y_val, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cb009-c710-421b-b3b7-4e9cff05442a",
   "metadata": {},
   "source": [
    "#### Histogram Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d9a92cf-1db4-45ed-8295-c098a2b40220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HistGB model...\n",
      "Starting training with 48 features...\n",
      "Binning 0.116 GB of training data: 0.797 s\n",
      "Binning 0.013 GB of validation data: 0.015 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.01439, val loss: 0.01617, in 0.032s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 9, train loss: 0.36686, val loss: 0.28434, in 0.031s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 12, train loss: 0.59063, val loss: 0.60107, in 0.035s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.54338, val loss: 0.57747, in 0.031s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 12, train loss: 0.54005, val loss: 0.55474, in 0.035s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 12, train loss: 0.52045, val loss: 0.53968, in 0.035s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 12, train loss: 0.49399, val loss: 0.50671, in 0.033s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 10, train loss: 0.38904, val loss: 0.33127, in 0.033s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 9, train loss: 0.35478, val loss: 0.31733, in 0.035s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 11, train loss: 0.27340, val loss: 0.30698, in 0.032s\n",
      "Fit 10 trees in 1.468 s, (310 total leaves)\n",
      "Time spent computing histograms: 0.136s\n",
      "Time spent finding best splits:  0.020s\n",
      "Time spent applying splits:      0.054s\n",
      "Time spent predicting:           0.007s\n"
     ]
    }
   ],
   "source": [
    "def train_hist_gb(X_train, y_train, X_val, y_val, rfe, random_state=42):\n",
    "    \"\"\"Train HistGB model\"\"\"\n",
    "    print(\"\\nTraining HistGB model...\")\n",
    "    print(f\"Starting training with {X_train.shape[1]} features...\")\n",
    "    \n",
    "    model = HistGradientBoostingClassifier(\n",
    "        max_iter=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=None,\n",
    "        random_state=random_state,\n",
    "        verbose=1\n",
    "    )\n",
    "    if rfe:\n",
    "        # Select optimal features using RFE\n",
    "        selected_features, train_scores, val_scores = select_optimal_features_rfe(X_train, y_train, X_val, y_val, model)\n",
    "        print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "        print(f\"Train scores: {train_scores}\")\n",
    "        print(f\"Validation scores: {val_scores}\")\n",
    "\n",
    "        # Select best features using embedded method\n",
    "        select_features, coef = select_best_features_embedded(X_train, y_train, model, 0.01)\n",
    "        print(f\"Selected {len(select_features)} features: {select_features}\")\n",
    "        print(f\"Feature importances: {coef}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "modelHGB = train_hist_gb(X_train, y_train, X_val, y_val, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be6f8-22c9-4d73-8f4e-b7f9b8d0f666",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ee6fca5-76d9-4f2c-b073-6e318e105ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Simple RF model...\n",
      "Starting training with 48 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   16.1s finished\n"
     ]
    }
   ],
   "source": [
    "def train_simple_rf(X_train, y_train, X_val, y_val, rfe, random_state=42):\n",
    "    \"\"\"Train Simple RF model\"\"\"\n",
    "    print(\"\\nTraining Simple RF model...\")\n",
    "    print(f\"Starting training with {X_train.shape[1]} features...\")\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=random_state,\n",
    "        n_jobs=2,\n",
    "        verbose=1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    if rfe:\n",
    "        # Select optimal features using RFE\n",
    "        selected_features, train_scores, val_scores = select_optimal_features_rfe(X_train, y_train, X_val, y_val, model)\n",
    "        print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "        print(f\"Train scores: {train_scores}\")\n",
    "        print(f\"Validation scores: {val_scores}\")\n",
    "\n",
    "        # Select best features using embedded method\n",
    "        select_features, coef = select_best_features_embedded(X_train, y_train, model, 0.01)\n",
    "        print(f\"Selected {len(select_features)} features: {select_features}\")\n",
    "        print(f\"Feature importances: {coef}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "modelRF = train_simple_rf(X_train, y_train, X_val, y_val, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025c161-79e1-4fad-9966-76040a62a03d",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07269c8d-3fe4-4951-a2a1-13e3acb937f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression model...\n",
      "Starting training with 48 features...\n"
     ]
    }
   ],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_val, y_val, rfe):\n",
    "    \"\"\"Train Logistic Regression model\"\"\"\n",
    "    print(\"\\nTraining Logistic Regression model...\")\n",
    "    print(f\"Starting training with {X_train.shape[1]} features...\")\n",
    "    \n",
    "    model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "    if rfe:\n",
    "        # Select optimal features using RFE\n",
    "        selected_features, train_scores, val_scores = select_optimal_features_rfe(X_train, y_train, X_val, y_val, model)\n",
    "        print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "        print(f\"Train scores: {train_scores}\")\n",
    "        print(f\"Validation scores: {val_scores}\")\n",
    "\n",
    "        # Select best features using embedded method\n",
    "        select_features, coef = select_best_features_embedded(X_train, y_train, model, 0.01)\n",
    "        print(f\"Selected {len(select_features)} features: {select_features}\")\n",
    "        print(f\"Feature importances: {coef}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "modelLR = train_logistic_regression(X_train, y_train, X_val, y_val, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9cf8a-1e2e-442a-8239-57bd0f276476",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a01e4669-fb19-4f68-ad9e-c1b087ec5f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Decision Tree model...\n",
      "Starting training with 48 features...\n"
     ]
    }
   ],
   "source": [
    "def train_decision_tree(X_train, y_train, X_val, y_val, rfe):\n",
    "    \"\"\"Train Decision Tree model\"\"\"\n",
    "    print(\"\\nTraining Decision Tree model...\")\n",
    "    print(f\"Starting training with {X_train.shape[1]} features...\")\n",
    "    \n",
    "    model = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "    if rfe:\n",
    "        # Select optimal features using RFE\n",
    "        selected_features, train_scores, val_scores = select_optimal_features_rfe(X_train, y_train, X_val, y_val, model)\n",
    "        print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "        print(f\"Train scores: {train_scores}\")\n",
    "        print(f\"Validation scores: {val_scores}\")\n",
    "\n",
    "        # Select best features using embedded method\n",
    "        select_features, coef = select_best_features_embedded(X_train, y_train, model, 0.01)\n",
    "        print(f\"Selected {len(select_features)} features: {select_features}\")\n",
    "        print(f\"Feature importances: {coef}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "modelDT = train_decision_tree(X_train, y_train, X_val, y_val, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc93550-7a3e-44c6-ac88-a277c69e70ac",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b40523ca-55f1-4cc9-9fc7-f83f37063ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para combinar GaussianNB e CategoricalNB\n",
    "def naive_bayes(X_num_train, X_cat_train, y_train, X_num_test, X_cat_test):\n",
    "    # Treinar GaussianNB para dados numéricos\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_num_train, y_train)\n",
    "    \n",
    "    # Treinar CategoricalNB para dados categóricos\n",
    "    cnb = CategoricalNB()\n",
    "    cnb.fit(X_cat_train, y_train)\n",
    "\n",
    "    # Predizer probabilidades com ambos os modelos\n",
    "    prob_gnb = gnb.predict_proba(X_num_test)\n",
    "    prob_cnb = cnb.predict_proba(X_cat_test)\n",
    "\n",
    "    # Combinar probabilidades multiplicando-as (assumindo independência)\n",
    "    combined_prob = prob_gnb * prob_cnb\n",
    "\n",
    "    # Retornar a classe com maior probabilidade combinada\n",
    "    return np.argmax(combined_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579cf4f-5df9-4613-a589-119cc29baac5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"evaluate\">\n",
    "    \n",
    "# 6.2 Evaluate the model\n",
    "    \n",
    "</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4030b83-2923-44b1-878b-deecaf65f502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94    164172\n",
      "           1       0.19      0.38      0.25      8036\n",
      "\n",
      "    accuracy                           0.89    172208\n",
      "   macro avg       0.58      0.65      0.60    172208\n",
      "weighted avg       0.93      0.89      0.91    172208\n",
      "\n",
      "F1-score: 0.5955\n",
      "\n",
      "Model: XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98    164172\n",
      "           1       0.93      0.04      0.08      8036\n",
      "\n",
      "    accuracy                           0.96    172208\n",
      "   macro avg       0.94      0.52      0.53    172208\n",
      "weighted avg       0.95      0.96      0.94    172208\n",
      "\n",
      "F1-score: 0.5306\n",
      "\n",
      "Model: HistGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    164172\n",
      "           1       0.62      0.04      0.07      8036\n",
      "\n",
      "    accuracy                           0.95    172208\n",
      "   macro avg       0.79      0.52      0.52    172208\n",
      "weighted avg       0.94      0.95      0.93    172208\n",
      "\n",
      "F1-score: 0.5240\n",
      "\n",
      "Model: RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98    164172\n",
      "           1       0.77      0.06      0.12      8036\n",
      "\n",
      "    accuracy                           0.96    172208\n",
      "   macro avg       0.86      0.53      0.55    172208\n",
      "weighted avg       0.95      0.96      0.94    172208\n",
      "\n",
      "F1-score: 0.5485\n",
      "\n",
      "Model: DT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98    164172\n",
      "           1       0.80      0.04      0.07      8036\n",
      "\n",
      "    accuracy                           0.95    172208\n",
      "   macro avg       0.88      0.52      0.52    172208\n",
      "weighted avg       0.95      0.95      0.93    172208\n",
      "\n",
      "F1-score: 0.5238\n",
      "\n",
      "Best model: LogisticRegression\n",
      "Best classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94    164172\n",
      "           1       0.19      0.38      0.25      8036\n",
      "\n",
      "    accuracy                           0.89    172208\n",
      "   macro avg       0.58      0.65      0.60    172208\n",
      "weighted avg       0.93      0.89      0.91    172208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(X_val, y_val, X_test):\n",
    "    models = {\n",
    "        'LogisticRegression': modelLR,\n",
    "        'XGB': modelXGB, \n",
    "        'HistGB': modelHGB,\n",
    "        'RF': modelRF,\n",
    "        # 'GaussianNB': modelGNB,\n",
    "        'DT': modelDT\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_report = None\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Model: {name}\")\n",
    "        y_pred = model.predict(X_val)\n",
    "        report = classification_report(y_val, y_pred)\n",
    "        print(report)\n",
    "        score = f1_score(y_val, y_pred, average='macro')\n",
    "        print(f\"F1-score: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_report = report\n",
    "\n",
    "    print(f\"Best model: {best_model.__class__.__name__}\")\n",
    "    print(\"Best classification report:\")\n",
    "    print(best_report)\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return test_pred, best_model\n",
    "\n",
    "test_pred, best_model = evaluate_models(X_val, y_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614243fd-f4d0-4097-9b77-8058885ec673",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"export\">\n",
    "    \n",
    "# 6.3 Export the predictor\n",
    "    \n",
    "</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0e488b6-99db-47d5-be51-41f27f38380a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Agreement_Reached = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9450aff-ee2d-4029-aba5-9fec25b12568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Agreement_Reached' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store Agreement_Reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fccb29-c4ba-40a3-a7b2-3f4cc0a54efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
